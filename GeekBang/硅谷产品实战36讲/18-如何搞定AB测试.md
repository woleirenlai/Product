# 18-如何搞定A/B测试

A/B测试是什么

A/B测试是用两组及以上随机分配、数量相似的样本进行对比，如果实验组和对比组的实验结果相比，在目标指标上具有统计显著性，那就可以说明实验组的功能可以导致你想要的结果，从而帮你验证假设或者做出产品决定。

小到一个按钮用红色还是黑色，达到有没有朋友圈这个功能对微信日活数的影响，都可以通过A/B测试来解决。A/B测试并不仅仅是对比一个功能前后的指标变化这么简单，错误往往隐藏在你认为最不容易出错的地方，想要最大程度地发挥A/B测试的作用，需要绕开一些坑。

## 进行 A/B 测试前，必须明确要测什么、如何测

无论采用什么测试方法，你都必须在产品测试的之前弄清楚要测试什么、怎么测，也就是说，在设计产品时要先从问题出发做出假设。

比如你可能会认为：微信中“加好友”这个按钮不好找，导致用户加好友的次数不够，我认为把这个按钮从屏幕上方改到下方，可以让体验更顺畅，以增加用户加好友的次数。

这时，你已经明确了需要测试什么、如何测，以及用什么指标来衡量的问题，接下来就可以进行 A/B 测试了。

通过对比“加好友”这个按钮在屏幕上方和下方时，用户使用次数的数据指标，来验证你观点的正确性。如果把这个按钮改到屏幕下方后，用户使用频率增加了，那就可以说明这样的改动达到了目的。

**如果你还不能 100% 清楚你要改变的是什么、提高的是什么，以及用什么指标来衡量，那么 A/B 测试并不适合你。**

## 验证因果性的唯一途径是 A/B 测试

首先明确相关性和因果性的概念：因果性是指，因为有 X，才会有 Y。

比如，因为用户没有内容可以看，所以他们流失了。相关性是指，X 变化会导致 Y 变化，但是不能明确 Y 的变化是由 X 的变化引起的。  
比如，你发现没有内容可以看的用户，流失的比例更高， 但是你不能确定是因为没有内容可以看，所以他们才流失的。

一个例子说明A/B测试如何验证因果性：一款 APP，数据显示启用了“隐私设置”的用户中，活跃用户的比例比较高。单单这个数据，只能说明用户活跃程度和“隐私设置”功能具有相关性，并不能说明这二者之间的因果关系。

这里的**相关性**是指，启用“隐私设置”功能的**更可能**是活跃用户，而**不能确定**说“隐私设置”功能可以提升用户的活跃程度。

而因果性，就是**明确了**用户开启“隐私设置”功能后，**就可以**提升他们的活跃程度。

这个因果关系形成的过程：比如用户启用了“隐私设置”功能后，他们可以控制谁能看到他们的内容，而不用再担心被不相关的人看到，所以他们会更放心、更大胆地发新内容，也就变得更活跃了。

设计A/B测试验证用户活跃程度和“隐私设置”功能间是否有因果关系：实验组用户可以使用“隐私设置”的功能，而对照组用户无法使用这个功能，其他实验条件完全一致。

结果，A/B 测试显示，“隐私设置”并不能提升用户活跃程度，二者并没有任何因果关系。而活跃用户启用“隐私设置”的比例比较高，原因竟是这个按钮不好找。

这个例子中，如果没有进行 A/B 测试，而草率地决定把“隐私设置”的按钮做得非常大， 鼓励更多用户启用“隐私设置”以提升他们的活跃程度，那最终的结果就是花费了大量的时间改设计、重新开发，却是“竹篮打水一场空”。

但是，如果 A/B 测试的结果证明用户活跃程度和“隐私设置”有因果关系，那么你就可以很自信地把“隐私设置”的功能设计得尽可能得显眼，以提升用户活跃度指标。

这个例子的经验：**某些功能的优化可能需要整个产品团队花费很多的时间、精力去改设计、重新开发，但是优化后并不能达到预期的效果，这种情况下，你可以先进行 A/B 测试，验证这个功能的优化与指标提升是否具有因果关系。在我看来，这就是 A/B 测试的魔力，它可以帮助你做出科学、合理的产品决定。**

## 明确到底需不需要 A/B 测试--样本数量

A/B 测试适用的场景是你的产品功能有多种选择，而你需要通过数据做出选择，这也就意味着并不是所有功能上线前都要经过 A/B 测试。

两种不适合 A/B 测试的情况：  
要测试某个按钮的颜色设置为红色、绿色、蓝色，还是紫色的效果好，那么进行 A/B 测试时，你就需要至少 4 组对比实验，而且要确保每一组实验都有足够的样本数量来保证对比结果具有统计显著性。  
如果这个按钮一共才 20 个人用，每组只有 5 个用户，那么得出的实验结果必然带有很大的偶然性，你无法根据这个实验数据做出科学的结论。这种情况下，你如果还要通过 A/B 测试做产品决定，那你就必须增加样本数量。

可以进行 A/B 测试的实验样本规模取决于实验组和对比组之间的区别到底有多大  
比如Facebook 的朋友圈（News Feed）是世界上最大的信息流产品，如果增加一个新功能可以让日活数增加 1%，那这个功能就是巨大的成功。这时A/B 测试需要的样本数量就非常大，才能保证这 1% 的进步具有统计显著性而不是误差。

但是，很多创业公司，它们的产品思路还没有定型，产品功能千变万化，有时一个新功能的发布可以将产品指标提升 100%。这种情况下，即使没有那么多的样本数量，你也可以肯定这个新功能可以给产品指标带来质的飞跃。

## 短期数据VS长期数据

如果微信的“摇一摇”突然出现在了微信开启页面里，那可以肯定，“摇一摇”功能的用户使用量会直线上升。这时设置一个 A/B 测试，实验组的“摇一摇”设置在微信开启页面，而对照组的“摇一摇”依然保留在“发现”页面，短期内肯定是实验组的数据更好看。

但是，如果你根据前两天的数据，就直接得出“摇一摇”在微信开启页面的效果会更高的结论，那我会觉得你这个产品经理是不可信的。原因是因为你没有意识到**新功能的短期新鲜感和长期的生态系统影响**。

微信开启页面突然出现“摇一摇”的功能，用户使用数据会因为刚开始的新鲜感而非常好看，用户这时正在劲儿头上。但是这个“好看”的数据并不一定可以延续，并不是每个新功能或者产品都可以延续这样的势头，大部分产品的新鲜感只能持续一个星期，最多一个月。

新鲜感的劲儿头过后，产品的数据会直线下降，可以说是成了“扶不起的阿斗”，再也提不上来了。有很多产品经理就是因为前几天好看的数据而过早地下了结论，最终产品发布后表现地远不如预期的好。

所以我的建议是，在判断一个功能是不是值得发布时，你应该等至少一个星期、短期的新鲜感褪去后， 再衡量是否值得发布。

还有，如果你通过 A/B 测试的结果决定要发布这个产品，我还建议你应该留一个长期的对比实验组，比如 1% 的用户无法使用新产品，来观察这个产品对整个生态系统产生的影响，并适时作出调整。

比如： Instagram 要增加一个给好友点“超级赞”的功能，目的是提高用户分享的频率。刚开始用户的活跃程度确实提高了，因为有了“超级赞”， 他们发新鲜事倍儿有精神，分享数量大幅度提升，短期数据棒极了。

但从长期来看，增加了“超级赞”的功能后，用户会因为只是得到了“赞”而没有获得“超级赞”，而感觉自尊心受损，最终不愿意也不敢分享了，所以从长期来看是数据下降了。